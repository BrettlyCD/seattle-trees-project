{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas:\n",
    "\n",
    "Forecast into future (change age at obs)\n",
    "Forecast rainy next 5 years (change age at obs and prcp)\n",
    "Forecast change in temp (change age at obs and temp)\n",
    "Idetify if a specific species it at risk\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Modeling\n",
    "\n",
    "## 5.0 Introduction\n",
    "\n",
    "Now that we've got our model established, I want to do some exploring. First, I will load in my full testing and training data and fit the model to the full range of data. I will unfortunately need to do the full imputing, SMOTE (to address imbalanced classes), scaling, and encoding to keep things consistent. \n",
    "\\\n",
    "\\\n",
    "Once we have that full sampling prepped, we can fit on it then predict using our *ENTIRE DATAFRAME*. That's a lot of data so let's hope it goes well. From there we can do some mapping and modeling. Should be fun!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import __version__ as sklearn_version\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Load and process\n",
    "\n",
    "### 5.2.0 Load Data\n",
    "Load in the raw X and y data we saved at the end of our last step. This is our full 20k sample records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 10) (20000, 1)\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv('../data/data_outputs/X_full_sample.csv')\n",
    "y = pd.read_csv('../data/data_outputs/y_full_sample.csv')\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy X and y to new dataframes\n",
    "X_full = X.reset_index(drop=True)\n",
    "y_full = y.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Process Data for Fitting Model\n",
    "\n",
    "#### 5.2.1.0 Initialized Imputers, Scalers, and Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "ss_scaler = StandardScaler()\n",
    "pow_trans = PowerTransformer()\n",
    "ohe1 = OneHotEncoder(handle_unknown='ignore', sparse_output=False) #one for our common_name column\n",
    "ohe2 = OneHotEncoder(handle_unknown='ignore', sparse_output=False) #one for our native column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.1 Create and Apply Name Grouping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_categories(df, col, n_limit):\n",
    "    \"\"\" Store categories in df[col] with counts less than the specified n and overwrite the corresponding values in the df with 'Other' \"\"\"\n",
    "    groups = df[col]\n",
    "    group_counts = groups.value_counts()\n",
    "    mask = groups.isin(group_counts[group_counts<n_limit].index)\n",
    "    df.loc[mask, col] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run group categories on common_name, here using 10 since the data is larger\n",
    "group_categories(X_full, 'common_name', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other                      1092\n",
       "Red maple                   696\n",
       "Apple/crabapple             566\n",
       "Purpleleaf plum variety     565\n",
       "Norway maple                542\n",
       "                           ... \n",
       "California hazel             10\n",
       "Holm (holly) oak             10\n",
       "River birch                  10\n",
       "Ponderosa pine               10\n",
       "Norway spruce                10\n",
       "Name: common_name, Length: 263, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View Value Counts\n",
    "X_full['common_name'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.2 Imput Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep for SMOTE, impute missing and group categorical values\n",
    "#impute missing values\n",
    "X_full['age_at_obs'] = num_imputer.fit_transform(X_full[['age_at_obs']])\n",
    "X_full['common_name'] = cat_imputer.fit_transform(X_full[['common_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 10)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.3 SMOTE X Data to Address Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "condition_index\n",
       "4.0                55.325\n",
       "3.0                24.135\n",
       "5.0                13.510\n",
       "2.0                 5.420\n",
       "1.0                 1.610\n",
       "dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View our balance of classes\n",
    "y_full.value_counts() / y_full.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use smote\n",
    "sm = SMOTENC(random_state=42, categorical_features=[0,2])\n",
    "X_smote, y_smote = sm.fit_resample(X_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition_index\n",
      "1.0                20.0\n",
      "2.0                20.0\n",
      "3.0                20.0\n",
      "4.0                20.0\n",
      "5.0                20.0\n",
      "dtype: float64\n",
      "(55325, 10)\n"
     ]
    }
   ],
   "source": [
    "#view new resampled distribution\n",
    "print(y_smote.value_counts() / y_smote.shape[0] * 100)\n",
    "\n",
    "print(X_smote.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.4 Scale Numerical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Scaler\n",
    "X_smote[['age_at_obs','norm_prcp_mm_total', 'temp_avg_normal','prcp_mm_normal']] = ss_scaler.fit_transform(X_smote[['age_at_obs','norm_prcp_mm_total', 'temp_avg_normal','prcp_mm_normal']])\n",
    "\n",
    "#Power Transformer\n",
    "X_smote[['diameter_breast_height_CM', 'norm_snow_mm_total','distance_between','adj_reports']] = pow_trans.fit_transform(X_smote[['diameter_breast_height_CM', 'norm_snow_mm_total','distance_between','adj_reports']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>common_name</th>\n",
       "      <th>diameter_breast_height_CM</th>\n",
       "      <th>native</th>\n",
       "      <th>age_at_obs</th>\n",
       "      <th>adj_reports</th>\n",
       "      <th>norm_prcp_mm_total</th>\n",
       "      <th>norm_snow_mm_total</th>\n",
       "      <th>distance_between</th>\n",
       "      <th>temp_avg_normal</th>\n",
       "      <th>prcp_mm_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Norwegian sunset maple</td>\n",
       "      <td>1.001507</td>\n",
       "      <td>introduced</td>\n",
       "      <td>0.351846</td>\n",
       "      <td>0.293119</td>\n",
       "      <td>-1.626061</td>\n",
       "      <td>0.576894</td>\n",
       "      <td>-0.767596</td>\n",
       "      <td>1.283766</td>\n",
       "      <td>-1.212323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Red maple</td>\n",
       "      <td>1.273186</td>\n",
       "      <td>introduced</td>\n",
       "      <td>0.628893</td>\n",
       "      <td>0.717425</td>\n",
       "      <td>-0.707730</td>\n",
       "      <td>-0.994286</td>\n",
       "      <td>-1.326386</td>\n",
       "      <td>-0.784582</td>\n",
       "      <td>0.770351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paperbark maple</td>\n",
       "      <td>-1.156845</td>\n",
       "      <td>introduced</td>\n",
       "      <td>-1.550908</td>\n",
       "      <td>-1.389654</td>\n",
       "      <td>-0.097643</td>\n",
       "      <td>-0.994286</td>\n",
       "      <td>0.911477</td>\n",
       "      <td>-0.784582</td>\n",
       "      <td>0.770351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cleveland norway maple</td>\n",
       "      <td>1.188151</td>\n",
       "      <td>introduced</td>\n",
       "      <td>-0.407873</td>\n",
       "      <td>0.868185</td>\n",
       "      <td>-1.212498</td>\n",
       "      <td>-0.994286</td>\n",
       "      <td>-0.628394</td>\n",
       "      <td>1.283766</td>\n",
       "      <td>-1.212323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Other</td>\n",
       "      <td>-0.741043</td>\n",
       "      <td>no_info</td>\n",
       "      <td>-2.177863</td>\n",
       "      <td>0.918547</td>\n",
       "      <td>0.795283</td>\n",
       "      <td>1.517000</td>\n",
       "      <td>0.496514</td>\n",
       "      <td>1.283766</td>\n",
       "      <td>-1.212323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              common_name  diameter_breast_height_CM      native  age_at_obs  \\\n",
       "0  Norwegian sunset maple                   1.001507  introduced    0.351846   \n",
       "1               Red maple                   1.273186  introduced    0.628893   \n",
       "2         Paperbark maple                  -1.156845  introduced   -1.550908   \n",
       "3  Cleveland norway maple                   1.188151  introduced   -0.407873   \n",
       "4                   Other                  -0.741043     no_info   -2.177863   \n",
       "\n",
       "   adj_reports  norm_prcp_mm_total  norm_snow_mm_total  distance_between  \\\n",
       "0     0.293119           -1.626061            0.576894         -0.767596   \n",
       "1     0.717425           -0.707730           -0.994286         -1.326386   \n",
       "2    -1.389654           -0.097643           -0.994286          0.911477   \n",
       "3     0.868185           -1.212498           -0.994286         -0.628394   \n",
       "4     0.918547            0.795283            1.517000          0.496514   \n",
       "\n",
       "   temp_avg_normal  prcp_mm_normal  \n",
       "0         1.283766       -1.212323  \n",
       "1        -0.784582        0.770351  \n",
       "2        -0.784582        0.770351  \n",
       "3         1.283766       -1.212323  \n",
       "4         1.283766       -1.212323  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_smote.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.5 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index to create clean join after encoding\n",
    "X_smote.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#transform our common_name field with ohe and put into dataframe\n",
    "X_name = ohe1.fit_transform(X_smote[['common_name']])\n",
    "name_df = pd.DataFrame(X_name, columns=ohe1.categories_[0]) #indexing 0 here to grab only the column names\n",
    "\n",
    "#transform our native field with ohe and put into dataframe\n",
    "X_native = ohe2.fit_transform(X_smote[['native']])\n",
    "native_df = pd.DataFrame(X_native, columns=ohe2.categories_[0])\n",
    "\n",
    "#concat these two dataframes back into X_res and X_test and create new dataframe, dropping original categorical fields\n",
    "X_smote_scaled_coded = pd.concat([X_smote, name_df, native_df], axis=1)\n",
    "\n",
    "#drop original categorical fields\n",
    "X_smote_scaled_coded.drop(columns=['common_name', 'native'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55325, 10) (55325, 264) (55325, 3)\n",
      "(55325, 275)\n",
      "(55325, 1)\n"
     ]
    }
   ],
   "source": [
    "#review shape of created variables\n",
    "print(X_smote.shape,name_df.shape,native_df.shape)\n",
    "print(X_smote_scaled_coded.shape)\n",
    "print(y_smote.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Fit Model on Our Full Data Sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2.0 Setup Model\n",
    "\n",
    "We'll use the same parameters as our final model from pre-processing. I didn't end up saving the model and importing due to the large size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=1000, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', max_depth=40, bootstrap=True, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2.1 Fit on Full 20k Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=40, n_estimators=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=40, n_estimators=1000, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=40, n_estimators=1000, random_state=42)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_smote_scaled_coded, y_smote.values.ravel()) #values.ravel will flatted the array."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2.2 View Classification Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.99\n",
      "Classification Report for Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00     11065\n",
      "         2.0       1.00      0.99      1.00     11065\n",
      "         3.0       0.99      0.99      0.99     11065\n",
      "         4.0       0.98      0.99      0.99     11065\n",
      "         5.0       0.99      0.99      0.99     11065\n",
      "\n",
      "    accuracy                           0.99     55325\n",
      "   macro avg       0.99      0.99      0.99     55325\n",
      "weighted avg       0.99      0.99      0.99     55325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy on training data: {accuracy_score(model.predict(X_smote_scaled_coded), y_smote):.2f}')\n",
    "print(\"Classification Report for Training Data\")\n",
    "print(classification_report(y_smote, model.predict(X_smote_scaled_coded)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've added in our full train and test set, I'm noticing our model is really accurate on the 1s and 2s when SMOTE-ed. Some further tuning might be helpful to make it more accurate on the more common classes, but I will leave it for now. We can move onto applying this model to our full data set. Woohoo!\n",
    "\n",
    "## 5.3 Apply Model to Full Data\n",
    "\n",
    "### 5.3.1 Load Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>common_name</th>\n",
       "      <th>diameter_breast_height_CM</th>\n",
       "      <th>native</th>\n",
       "      <th>age_at_obs</th>\n",
       "      <th>condition_index</th>\n",
       "      <th>adj_reports</th>\n",
       "      <th>norm_prcp_mm_total</th>\n",
       "      <th>norm_snow_mm_total</th>\n",
       "      <th>distance_between</th>\n",
       "      <th>temp_avg_normal</th>\n",
       "      <th>prcp_mm_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(european) white birch</td>\n",
       "      <td>40.64</td>\n",
       "      <td>introduced</td>\n",
       "      <td>27.765115</td>\n",
       "      <td>5.0</td>\n",
       "      <td>237</td>\n",
       "      <td>1071.925479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.947927</td>\n",
       "      <td>53.2</td>\n",
       "      <td>960.628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kwanzan flowering cherry</td>\n",
       "      <td>5.08</td>\n",
       "      <td>no_info</td>\n",
       "      <td>27.743212</td>\n",
       "      <td>3.0</td>\n",
       "      <td>237</td>\n",
       "      <td>1071.925479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.367105</td>\n",
       "      <td>53.2</td>\n",
       "      <td>960.628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japanese snowbell tree</td>\n",
       "      <td>2.54</td>\n",
       "      <td>introduced</td>\n",
       "      <td>27.756901</td>\n",
       "      <td>5.0</td>\n",
       "      <td>237</td>\n",
       "      <td>1071.925479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.145690</td>\n",
       "      <td>53.2</td>\n",
       "      <td>960.628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                common_name  diameter_breast_height_CM      native  \\\n",
       "0    (european) white birch                      40.64  introduced   \n",
       "1  Kwanzan flowering cherry                       5.08     no_info   \n",
       "2    Japanese snowbell tree                       2.54  introduced   \n",
       "\n",
       "   age_at_obs  condition_index  adj_reports  norm_prcp_mm_total  \\\n",
       "0   27.765115              5.0          237         1071.925479   \n",
       "1   27.743212              3.0          237         1071.925479   \n",
       "2   27.756901              5.0          237         1071.925479   \n",
       "\n",
       "   norm_snow_mm_total  distance_between  temp_avg_normal  prcp_mm_normal  \n",
       "0                 0.0          0.947927             53.2         960.628  \n",
       "1                 0.0          3.367105             53.2         960.628  \n",
       "2                 0.0          1.145690             53.2         960.628  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seattle_trees_all = pd.read_csv('../data/data_outputs/seattle_trees_full.csv')\n",
    "\n",
    "seattle_trees_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158004, 11)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seattle_trees_all.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1.1 Split Dependent and Independent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_X = seattle_trees_all.drop(columns='condition_index')\n",
    "seattle_y = seattle_trees_all['condition_index']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Pre-Process Data\n",
    "\n",
    "#### 5.3.2.0 Group Name Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run group categories on common_name, here using 10 since the data is larger\n",
    "group_categories(seattle_X, 'common_name', 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2.1 Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing values\n",
    "seattle_X['age_at_obs'] = num_imputer.transform(seattle_X[['age_at_obs']])\n",
    "seattle_X['common_name'] = cat_imputer.transform(seattle_X[['common_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158004, 10)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seattle_X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2.2 Scale Numerical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Scaler\n",
    "seattle_X[['age_at_obs','norm_prcp_mm_total', 'temp_avg_normal','prcp_mm_normal']] = ss_scaler.transform(seattle_X[['age_at_obs','norm_prcp_mm_total', 'temp_avg_normal','prcp_mm_normal']])\n",
    "\n",
    "#Power Transformer\n",
    "seattle_X[['diameter_breast_height_CM', 'norm_snow_mm_total','distance_between','adj_reports']] = pow_trans.transform(seattle_X[['diameter_breast_height_CM', 'norm_snow_mm_total','distance_between','adj_reports']])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2.3 Encode Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index to create clean join after encoding\n",
    "seattle_X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#transform our common_name field with ohe and put into dataframe\n",
    "seattle_name = ohe1.transform(seattle_X[['common_name']])\n",
    "seattle_name_df = pd.DataFrame(seattle_name, columns=ohe1.categories_[0]) #indexing 0 here to grab only the column names\n",
    "\n",
    "#transform our native field with ohe and put into dataframe\n",
    "seattle_native = ohe2.transform(seattle_X[['native']])\n",
    "seattle_native_df = pd.DataFrame(seattle_native, columns=ohe2.categories_[0])\n",
    "\n",
    "#concat these two dataframes back into X_res and X_test and create new dataframe, dropping original categorical fields\n",
    "seattle_scaled_coded = pd.concat([seattle_X, seattle_name_df, seattle_native_df], axis=1)\n",
    "\n",
    "#drop original categorical fields\n",
    "seattle_scaled_coded.drop(columns=['common_name', 'native'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158004, 10) (158004, 264) (158004, 3)\n",
      "(158004, 275)\n",
      "(158004,)\n"
     ]
    }
   ],
   "source": [
    "#review shape of created variables\n",
    "print(seattle_X.shape,seattle_name_df.shape,seattle_native_df.shape)\n",
    "print(seattle_scaled_coded.shape)\n",
    "print(seattle_y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Run Model on Full Seattle Data And Review Results\n",
    "\n",
    "#### 5.3.3.0 Predict Using Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.65\n",
      "Classification Report for Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.21      0.28      0.24      2806\n",
      "         2.0       0.27      0.30      0.28      8268\n",
      "         3.0       0.53      0.49      0.51     37485\n",
      "         4.0       0.77      0.75      0.76     87557\n",
      "         5.0       0.61      0.67      0.64     21888\n",
      "\n",
      "    accuracy                           0.65    158004\n",
      "   macro avg       0.48      0.50      0.49    158004\n",
      "weighted avg       0.65      0.65      0.65    158004\n",
      "\n",
      "----------------------------------------\n",
      "[[  775   306   587   809   329]\n",
      " [  365  2472  2461  2351   619]\n",
      " [  890  2972 18510 12402  2711]\n",
      " [ 1321  2978 11789 65814  5655]\n",
      " [  343   499  1840  4543 14663]]\n"
     ]
    }
   ],
   "source": [
    "#predict using tune model\n",
    "y_pred = model.predict(seattle_scaled_coded)\n",
    "\n",
    "print(f'Accuracy on training data: {accuracy_score(y_pred, seattle_y):.2f}')\n",
    "print(\"Classification Report for Training Data\")\n",
    "print(classification_report(seattle_y, y_pred))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(seattle_y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
